import torch
import torch.nn as nn
import torch.nn.functional as F
import math
from typing import Optional, Tuple
from .config import MiniMindConfig
from .RoPE import apply_rotary_pos_emb

def repeat_kv(x: torch.Tensor, n_rep: int):
    bs, seq, num_key_value_heads, head_dim = x.shape
    if n_rep == 1:
        return x
    return (
        x[:,:,:,None,:]
        .expand(bs, seq, num_key_value_heads, n_rep, head_dim)
        .reshape(bs, seq, num_key_value_heads*n_rep, head_dim)
    )

class Attention(nn.Module):
    def __init__(self, args: MiniMindConfig):
        super().__init__()
        
        self.num_key_value_heads = args.num_key_value_heads if args.num_key_value_heads is not None else args.num_attention_heads
        assert args.num_attention_heads % args.num_key_value_heads == 0
        
        self.n_local_heads = args.num_attention_heads
        self.n_rep = args.num_attention_heads // self.num_key_value_heads
        self.head_dim = args.hidden_size // args.num_attention_heads
        
        # n_rep个头共享一组k和v
        # 后续在通过重复xk和xv来达到跟xq一样的shape
        self.q_proj = nn.Linear(args.hidden_size, args.num_attention_heads*self.head_dim, bias=False)
        self.k_proj = nn.Linear(args.hidden_size, self.num_key_value_heads*self.head_dim, bias=False)
        self.v_proj = nn.Linear(args.hidden_size, self.num_key_value_heads*self.head_dim, bias=False)
        self.o_proj = nn.Linear(args.num_attention_heads*self.head_dim, args.hidden_size, bias=False)
        
        self.dropout = args.dropout        
        self.attn_dropout = nn.Dropout(args.dropout)
        self.resid_dropout = nn.Dropout(args.dropout)
        
        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention') and args.flash_attention
        
    def forward(
        self,
        x: torch.Tensor,
        position_embeding: Tuple[torch.Tensor, torch.Tensor],
        past_key_value: Optional[Tuple[torch.Tensor, torch.Tensor]],
        use_cache: bool=False,
        attention_mask: Optional[torch.Tensor]=None
    ):
        # 投影计算qkv
        bs, seq_len, _ = x.shape
        xq, xk, xv = self.q_proj(x), self.k_proj(x), self.v_proj(x)
        # 把输入拆分成多个注意力头
        xq = xq.view(bs, seq_len, self.n_local_heads, self.head_dim)
        xk = xk.view(bs, seq_len, self.num_key_value_heads, self.head_dim)
        xv = xv.view(bs, seq_len, self.num_key_value_heads, self.head_dim)
        # q和k用roPE
        cos, sin = position_embeding
        xq, xk = apply_rotary_pos_emb(xq, xk, cos, sin)
        # k和v用repeat
        if past_key_value is not None:
            # kvcache机制
            xk=torch.cat([past_key_value[0],xk], dim=1) # 在seq维度上拼接
            xv=torch.cat([past_key_value[1],xv], dim=1)
        past_kv=(xk, xv) if use_cache else None
        xq, xk, xv=(
            xq.transpose(1,2),
            repeat_kv(xk, self.n_rep).transpose(1,2),
            repeat_kv(xv, self.n_rep).transpose(1,2)  
        )
        # 使用attention进行计算
        if self.flash and seq_len>1 and (attention_mask is None or torch.all(attention_mask==1)):
            attn_mask = (
                None
                if attention_mask is None
                else attention_mask.view(bs, 1, 1, -1).expand(bs, self.n_local_heads, seq_len, -1).bool()
            )
            output = F.scaled_dot_product_attention(
                xq, xk, xv,
                attn_mask=attn_mask,
                dropout_p=self.dropout if self.training else 0.0,is_causal=True
            )
        else:
            scores = (xq @ xk.transpose(-2, -1) / math.sqrt(self.head_dim))
            # 添加掩码
            # mask: [seq_len, seq_len]->[1, 1, seq_len, seq_len]
            scores = scores + torch.triu(
                torch.full((seq_len, seq_len), float("-inf"), device = scores.device, diagonal=1)
            ).unsqueeze(0).unsqueeze(0) # 理论上可以不用写unsqueeze，会从右到左自动广播
            # 外部的掩码，比如padding掩码
            if attention_mask is not None:
                # padding掩码的shape为[bs, seq_len]->[bs, 1, 1, seq_len]，没懂为什么是1和2
                extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)
                extended_attention_mask = (1.0-extended_attention_mask) * -1e9
                scores = scores + extended_attention_mask
            
            scores = F.softmax(scores.float(), dim=-1).type_as(xq)
            scores = self.attn_dropout(scores)
            output = scores @ xv
        # 拼接头，输出投影
        output = output.transpose(1, 2).view(bs, seq_len, -1)
        output = self.resid_dropout(self.o_proj(output))
        return output, past_kv # 当前的kvcache
            